{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sagnik-Nandi/PDFQueryBot---Chatbot-over-PDFs-using-RAG/blob/main/assnmt%201%20-%20Digit%20recognition%20using%20Pytorch/digit_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "ovKLIqOlu6qM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before running, YOU need to create a 'data' directory in the current directory."
      ],
      "metadata": {
        "id": "arQl5nzc12x7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Data Exploration\n",
        "- Importing pytorch and other libs\n",
        "- Create dataset and dataloader instances\n"
      ],
      "metadata": {
        "id": "RNord9JGvEAV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "W6MeVhr1lTp5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the dataset\n",
        "train=datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        ")\n",
        "test=datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        ")"
      ],
      "metadata": {
        "id": "_PHMOiSLx7xs"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some idea about the dataset\n",
        "print(train)\n",
        "print(test)\n",
        "print(train[0][0].shape)\n",
        "print(train[0][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtnZwFxkJZNb",
        "outputId": "3d0d0263-6192-4ccf-82f5-678215e5cc04"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
            "           )\n",
            "Dataset MNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: data\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
            "           )\n",
            "torch.Size([1, 28, 28])\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and validation split   \n",
        "- create dataloader instances for each dataset\n",
        "- Hyperparameter used:\n",
        "1. Batch_size = 8   \n",
        "Reducing batch_size significantly improved accuracy, but too small batch_size makes the model computationally large and unstable\n",
        "- train-validation split ratio = 11:1   \n",
        "(changing split ratio did not affect accuracy much)"
      ],
      "metadata": {
        "id": "VNY7MH3tuvoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainAll=train\n",
        "train, eval=random_split(trainAll, [55000, 5000])\n",
        "\n",
        "# Hyperparameter: Batch_size\n",
        "batch_size=8\n",
        "\n",
        "# Dataloader for : train, eval, test\n",
        "train_loader=torch.utils.data.DataLoader(\n",
        "    train,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "  )\n",
        "eval_loader=torch.utils.data.DataLoader(\n",
        "    eval,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "  )\n",
        "test_loader=torch.utils.data.DataLoader(\n",
        "    test,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "  )\n"
      ],
      "metadata": {
        "id": "Izjdvdb0HjvW"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# REF: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "# Shows sample images from the dataset using plt.imshow\n",
        "def sampleData(data):\n",
        "  figure = plt.figure(figsize=(8, 8))\n",
        "  cols, rows = 3, 3\n",
        "  for i in range(1, cols * rows + 1):\n",
        "      sample_idx = torch.randint(len(data), size=(1,)).item()\n",
        "      img, label = data[sample_idx]\n",
        "      figure.add_subplot(rows, cols, i)\n",
        "      plt.title(label)\n",
        "      plt.axis(\"off\")\n",
        "      plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "  plt.show()\n",
        "# sampleData(train)"
      ],
      "metadata": {
        "id": "dd7tdsYxLG7z"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make a Neural Network model\n",
        "- Create model and forward function   \n",
        "- The model consists of: input layer(size 28*28), 1 hidden layer(size 32), and output layer(size 10)   \n",
        "(output layer has 10 values, for i in (0, 9) : Pr(input==i). The one having highest probability corresponds to the identified digit)\n",
        "- ReLU Activation function and Batch Normalization are used to enhance performance\n",
        "- Hyperparameters used:\n",
        "2. no. of hidden layers = 1   \n",
        "(increasing hidden layers does not improve accuracy)\n",
        "2. no. of units in layer1 = 32   \n",
        "(changing number of units does not affect accuracy)"
      ],
      "metadata": {
        "id": "LpJsRtOSzu9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten() # flattens 28*28 2d array to 1d vector\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # forward propagation function\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "model=NeuralNetwork()"
      ],
      "metadata": {
        "id": "kuq_oy1M3H7B"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and testing\n",
        " - Hyperparameters used:\n",
        "4. learning_rate = 0.001   \n",
        "  Critical parameter\n",
        "5. epoch = 10 (for testing and tuning purposes, set to 5)   \n",
        "  Critical parameter, increasing no. of epochs improved accuracy. But too large no. of epochs can be computaionally large and may lead to overfitting\n",
        "\n",
        "- Loss Function   \n",
        "Cross Entropy function is generally suitable in discrete-valued prediction problem, such as this one\n",
        "\n",
        "- Optimizer   \n",
        "Optimizer compiles the backward propagation step and update the parameters based on different algorithms, such as Gradient Descent   \n",
        "Tested with different ones like SGD(Stochastic Gradient Descent), ADAM() etc."
      ],
      "metadata": {
        "id": "2FlFY0R4FYrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters:\n",
        "learning_rate = 1e-3\n",
        "# batch_size = 8\n",
        "epochs = 5\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "dw5xvDqcBE5p"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# REF: https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer, verbose=True):\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation, update parameters and reset all the gradients to zero\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Show progress report :D\n",
        "        if verbose and batch % 500 == 0:\n",
        "            loss, current = loss.item(), batch * batch_size + len(X)\n",
        "            print(f\"Training Loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def eval_loop(dataloader, model, loss_fn):\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    valid_loss, correct = 0, 0\n",
        "\n",
        "    # torch.no_grad() ensures that no gradients are computed during eval mode\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            valid_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    valid_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Validation Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {valid_loss:>8f} \\n\")\n",
        "    return valid_loss"
      ],
      "metadata": {
        "id": "zLRUccGs_nD4"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# REF: https://www.geeksforgeeks.org/training-neural-networks-with-validation-using-pytorch/\n",
        "\n",
        "def fit(model, epochs, train_loader, eval_loader, loss_fn, optimizer, verbose=True):\n",
        "  min_valid_loss = np.inf\n",
        "  for t in range(epochs):\n",
        "      print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "      train_loop(train_loader, model, loss_fn, optimizer, verbose)\n",
        "      valid_loss = eval_loop(eval_loader, model, loss_fn)\n",
        "\n",
        "      # Saving parameters\n",
        "      if min_valid_loss > valid_loss:\n",
        "          print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\n Saving The Model\\n')\n",
        "          min_valid_loss = valid_loss\n",
        "          torch.save(model.state_dict(), 'saved_model.pth')\n",
        "  print(\"Done!\")\n",
        "\n",
        "fit(model, epochs, train_loader, eval_loader, loss_fn, optimizer, verbose=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJV9QlJXB8Zm",
        "outputId": "f7c217f3-1117-4f12-e1b0-263239cb8acf"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Validation Error: \n",
            " Accuracy: 93.3%, Avg loss: 0.232855 \n",
            "\n",
            "Validation Loss Decreased(inf--->0.232855) \n",
            " Saving The Model\n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Validation Error: \n",
            " Accuracy: 94.1%, Avg loss: 0.194673 \n",
            "\n",
            "Validation Loss Decreased(0.232855--->0.194673) \n",
            " Saving The Model\n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Validation Error: \n",
            " Accuracy: 95.0%, Avg loss: 0.175958 \n",
            "\n",
            "Validation Loss Decreased(0.194673--->0.175958) \n",
            " Saving The Model\n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Validation Error: \n",
            " Accuracy: 95.0%, Avg loss: 0.169466 \n",
            "\n",
            "Validation Loss Decreased(0.175958--->0.169466) \n",
            " Saving The Model\n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Validation Error: \n",
            " Accuracy: 95.4%, Avg loss: 0.155475 \n",
            "\n",
            "Validation Loss Decreased(0.169466--->0.155475) \n",
            " Saving The Model\n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def score(model, test_loader, loss_fn):\n",
        "  model.load_state_dict(torch.load('saved_model.pth'))\n",
        "  model.eval()\n",
        "  correct=0\n",
        "  size=len(test_loader.dataset)\n",
        "\n",
        "  for X, y in test_loader:\n",
        "    pred = model(X)\n",
        "    correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "  correct /= size\n",
        "  print(f\"Test score: \\n Accuracy: {(100*correct):>0.1f}%\")\n",
        "\n",
        "score(model, test_loader, loss_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe-LmtB7Bq9l",
        "outputId": "da9e6067-32b5-40ec-807b-4ce4bd4f5c79"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test score: \n",
            " Accuracy: 96.2%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}